{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4488bf0",
   "metadata": {},
   "source": [
    "# 门控机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b66ce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:44:09.166438300Z",
     "start_time": "2023-10-25T08:44:08.886510400Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] 找不到指定的程序。 Error loading \"D:\\conda\\envs\\Deeplearning\\Lib\\site-packages\\torch\\lib\\nvfuser_codegen.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 导入必要的库\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\envs\\Deeplearning\\Lib\\site-packages\\torch\\__init__.py:122\u001B[0m\n\u001B[0;32m    120\u001B[0m     err \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mWinError(last_error)\n\u001B[0;32m    121\u001B[0m     err\u001B[38;5;241m.\u001B[39mstrerror \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m Error loading \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdll\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m or one of its dependencies.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     is_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mOSError\u001B[0m: [WinError 127] 找不到指定的程序。 Error loading \"D:\\conda\\envs\\Deeplearning\\Lib\\site-packages\\torch\\lib\\nvfuser_codegen.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from d2l import torch as d2l\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7303a",
   "metadata": {},
   "source": [
    "首先导入数据集，借助d2l库可以直接导入时光机书数据集，这个库也提供了一些可视化函数；可以通过pip命令下载此库。复制第6章的数据集读取代码也可以达到同样效果（不建议）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f8396",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.981802200Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6b536",
   "metadata": {},
   "source": [
    "## 长短期记忆网络\n",
    "\n",
    "长短期忆网络的设计灵感来自于计算机的逻辑门。 其中一个门用来从单元中输出条目，称为输出门。另外一个门用来决定何时将数据读入单元，称为输入门。还需要一种机制来重置单元的内容，即遗忘门。<br>首先，让我们自己写一个粗略的长短期记忆网络的前向计算过程，但我们并不会用这个类来训练和预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1468de2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.982803200Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # 初始化模型，即各个门的计算参数\n",
    "        self.W_i = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_f = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_o = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_a = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.U_a = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.randn(1, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.randn(1, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.randn(1, hidden_size))\n",
    "        self.b_a = nn.Parameter(torch.randn(1, hidden_size))\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.randn(1, hidden_size))\n",
    "    # 初始化隐藏状态\n",
    "    def init_state(self, batch_size):\n",
    "        hidden_state = torch.zeros(batch_size, self.hidden_size)\n",
    "        cell_state = torch.zeros(batch_size, self.hidden_size)\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "    def forward(self, inputs, states=None):\n",
    "        batch_size, seq_len, input_size = inputs.shape\n",
    "        if states is None:\n",
    "            states = self.init_state(batch_size)\n",
    "        hidden_state, cell_state = states\n",
    "        outputs = []\n",
    "        for step in range(seq_len):\n",
    "            inputs_step = inputs[:, step, :]\n",
    "            i_gate = torch.sigmoid(torch.mm(inputs_step, self.W_i) + torch.mm(hidden_state, self.U_i) + self.b_i)\n",
    "            f_gate = torch.sigmoid(torch.mm(inputs_step, self.W_f) + torch.mm(hidden_state, self.U_f) + self.b_f)\n",
    "            o_gate = torch.sigmoid(torch.mm(inputs_step, self.W_o) + torch.mm(hidden_state, self.U_o) + self.b_o)\n",
    "            c_tilde = torch.tanh(torch.mm(inputs_step, self.W_a) + torch.mm(hidden_state, self.U_a) + self.b_a)\n",
    "            cell_state = f_gate * cell_state + i_gate * c_tilde\n",
    "            hidden_state = o_gate * torch.tanh(cell_state)\n",
    "            y = torch.mm(hidden_state, self.W_h)+ self.b_h\n",
    "            outputs.append(y)\n",
    "        return torch.cat(outputs, dim=0), (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf52a3",
   "metadata": {},
   "source": [
    "可以看出，长短时记忆网络模型的架构与基本的循环神经网络单元是相同的，需要对每个时间步分别计算前向过程，但权重更新公式更为复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceaaf3a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.983803Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.num_directions = 1\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "# 在第一个时间步，需要初始化一个隐藏状态，由此函数实现\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ced10d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.985802900Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    \n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print('Train Done!')\n",
    "    torch.save(net.state_dict(),'chapter6.pth')\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "\n",
    "\n",
    "# 在notebook中，若在训练后直接预测，会直接使用训练后的参数。\n",
    "# 若想保存模型参数，请解除注释下面的代码，在实例化新的模型进行预测时，请记得读入模型。\n",
    "# 读入模型的方法请参考前几章实现的Runner类。\n",
    "\n",
    "#     torch.save(net.state_dict(), 'chapter6.pth')\n",
    "\n",
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "            state.detach_()\n",
    "        else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "            for s in state:\n",
    "                s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * d2l.size(y), d2l.size(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
    "\n",
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.reshape(torch.tensor(\n",
    "        [outputs[-1]], device=device), (1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "\n",
    "def grad_clipping(net, theta): \n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b7621",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.986804500Z"
    }
   },
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\"\"\"\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f0f03",
   "metadata": {},
   "source": [
    "高级API包含了之前介绍的所有配置细节，所以我们可以直接实例化门控循环单元模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18e07b",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.987803800Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, num_epochs, lr= 28, 256, 200, 1\n",
    "device = try_gpu()\n",
    "lstm_layer = nn.LSTM(vocab_size, num_hiddens)\n",
    "model_lstm = RNNModel(lstm_layer, vocab_size)\n",
    "train(model_lstm, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9433c",
   "metadata": {},
   "source": [
    "## 门控循环单元（GRU）\n",
    "门控循环单元是一个稍微简化的变体，通常能够提供同等的效果，并且计算的速度明显更快。<br>我们自己写一个粗略长短期记忆网络的前向计算过程，我们更换了一种写法，用函数来实现这个功能，但我们依旧并不会用这个来训练和预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0f297",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.988804Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化各个“门”的参数\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(inputs, hiddens):\n",
    "        ctx = device\n",
    "        param = torch.rand((inputs, hiddens))\n",
    "        param.to(ctx)\n",
    "        return param\n",
    "    \n",
    "    def three():\n",
    "        return (normal(num_inputs, num_hiddens),\n",
    "                normal(num_hiddens, num_hiddens),\n",
    "                torch.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = three()  # 候选隐状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal(num_hiddens, num_outputs)\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "# 初始化隐藏状态，作为step=0时的输入\n",
    "def init_gru_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "# 门控单元\n",
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # @符号为矩阵乘法的运算符号\n",
    "    for X in inputs:\n",
    "        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
    "        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
    "        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bc47a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.989803400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 此代码使用手动实现的GRU函数\n",
    "model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
    "                            init_gru_state, gru)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8adb4e",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.990802900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 此代码调用Pytorch库的GRU类\n",
    "gru_layer = nn.GRU(vocab_size, num_hiddens)\n",
    "model_gru = RNNModel(gru_layer, vocab_size)\n",
    "train(model_gru, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f615a",
   "metadata": {},
   "source": [
    "上述代码既完成了包含LSTM层的循环神经网络的训练，也完成了包含GRU的循环神经网络的训练，所以可以分别使用训练过的网络来看看预测结果。‘time’是指定的开始的词语，也可以换成其他英文单词，甚至是乱码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75efde",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.992803100Z"
    }
   },
   "outputs": [],
   "source": [
    "predict('time ', 10, model_gru, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4729d0a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.993803700Z"
    }
   },
   "outputs": [],
   "source": [
    "predict('time ', 10, model_lstm, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb960f77",
   "metadata": {},
   "source": [
    "## 第七章- 优化算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e035f7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.994803Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入需要的工具包\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "from sklearn.datasets import load_iris\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd532b",
   "metadata": {},
   "source": [
    "使用第四章定义的Runner类, 但在本章中不涉及训练中的验证过程."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281499be",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.995802700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Runner(object):\n",
    "    def __init__(self, model, optimizer, loss_fn, metric=None, **kwargs):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        # 用于计算评价指标\n",
    "        self.metric = metric\n",
    "        \n",
    "        # 记录训练过程中的评价指标变化\n",
    "        self.dev_scores = []\n",
    "        # 记录训练过程中的损失变化\n",
    "        self.train_step_losses = []\n",
    "        self.dev_losses = []\n",
    "        # 记录全局最优评价指标\n",
    "        self.best_score = 0\n",
    "   \n",
    " \n",
    "# 模型训练阶段\n",
    "    def train(self, train_loader, dev_loader=None, **kwargs):\n",
    "        # 将模型设置为训练模式，此时模型的参数会被更新\n",
    "        self.model.train()\n",
    "        \n",
    "        num_epochs = kwargs.get('num_epochs', 0)\n",
    "        log_steps = kwargs.get('log_steps', 100)\n",
    "        save_path = kwargs.get('save_path','final_model.pth')\n",
    "        eval_steps = kwargs.get('eval_steps', 0)\n",
    "        # 运行的step数，不等于epoch数\n",
    "        global_step = 0\n",
    "        \n",
    "        if eval_steps:\n",
    "            if dev_loader is None:\n",
    "                raise RuntimeError('Error: dev_loader can not be None!')\n",
    "            if self.metric is None:\n",
    "                raise RuntimeError('Error: Metric can not be None')\n",
    "                \n",
    "        # 遍历训练的轮数\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            # 遍历数据集\n",
    "            for step, data in enumerate(train_loader):\n",
    "                x, y = data\n",
    "                logits = self.model(x.float())\n",
    "                loss = self.loss_fn(logits, y.long())\n",
    "                total_loss += loss\n",
    "                if log_steps and global_step%log_steps == 0:\n",
    "                    print(f'loss:{loss.item():.5f}')\n",
    "                    \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                # 保存当前轮次训练损失的累计值\n",
    "                self.train_step_losses.append((global_step,loss.item()))\n",
    "                \n",
    "            # 每隔一定轮次进行一次验证，由eval_steps参数控制，可以采用不同的验证判断条件\n",
    "            if eval_steps:\n",
    "                if (epoch+1)% eval_steps ==  0:\n",
    "\n",
    "                    dev_score, dev_loss = self.evaluate(dev_loader, global_step=global_step)\n",
    "                    print(f'[Evalute] dev score:{dev_score:.5f}, dev loss:{dev_loss:.5f}')\n",
    "                \n",
    "                    if dev_score > self.best_score:\n",
    "                        self.save_model(f'model_{epoch+1}.pth')\n",
    "                    \n",
    "                        print(f'[Evaluate]best accuracy performance has been updated: {self.best_score:.5f}-->{dev_score:.5f}')\n",
    "                        self.best_score = dev_score\n",
    "                    \n",
    "                # 验证过程结束后，请记住将模型调回训练模式   \n",
    "                    self.model.train()\n",
    "            \n",
    "\n",
    "            \n",
    "        self.save_model(save_path) \n",
    "        print('[Train] Train done')\n",
    "      \n",
    "    # 模型评价阶段\n",
    "    def evaluate(self, dev_loader, **kwargs):\n",
    "        assert self.metric is not None\n",
    "        # 将模型设置为验证模式，此模式下，模型的参数不会更新\n",
    "        self.model.eval()\n",
    "        global_step = kwargs.get('global_step',-1)\n",
    "        total_loss = 0\n",
    "        self.metric.reset()\n",
    "        \n",
    "        for batch_id, data in enumerate(dev_loader):\n",
    "            x, y = data\n",
    "            logits = self.model(x.float())\n",
    "            loss = self.loss_fn(logits, y.long()).item()\n",
    "            total_loss += loss \n",
    "            self.metric.update(logits, y)\n",
    "            \n",
    "        dev_loss = (total_loss/len(dev_loader))\n",
    "        self.dev_losses.append((global_step, dev_loss))\n",
    "        dev_score = self.metric.accumulate()\n",
    "        self.dev_scores.append(dev_score)\n",
    "        return dev_score, dev_loss\n",
    "    \n",
    "    # 模型预测阶段，\n",
    "    def predict(self, x, **kwargs):\n",
    "        self.model.eval()\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "    \n",
    "    # 保存模型的参数\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.model.state_dict(),save_path)\n",
    "        \n",
    "    # 读取模型的参数\n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba196c22",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.996803100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "178ff25e",
   "metadata": {},
   "source": [
    "使用第四章定义的可视化函数,观察不同超参数设置下的实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faed480",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.997802300Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training_loss_acc(runner, fig_name, fig_size=(16, 6), sample_step=20, loss_legend_loc='upper right', acc_legend_loc='lower right',\n",
    "                          train_color = '#8E004D', dev_color = '#E20079', fontsize='x-large', train_linestyle='-', dev_linestyle='--'):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.subplot(1,2,1)\n",
    "    train_items = runner.train_step_losses[::sample_step]\n",
    "    train_steps = [x[0] for x in train_items]\n",
    "    train_losses = [x[1] for x in train_items]\n",
    "    \n",
    "    plt.plot(train_steps, train_losses, color=train_color, linestyle=train_linestyle, label='Train loss')\n",
    "    if len(runner.dev_losses) > 0:\n",
    "        dev_steps = [x[0] for x in runner.dev_losses]\n",
    "        dev_losses = [x[1] for x in runner.dev_losses]\n",
    "        plt.plot(dev_steps, dev_losses, color=dev_color, linestyle=dev_linestyle,label='dev loss')\n",
    "    \n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('step')\n",
    "    plt.legend(loc=loss_legend_loc)\n",
    "    if len(runner.dev_scores) > 0:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(dev_steps, runner.dev_scores, color=dev_color, linestyle=dev_linestyle, label='dev accuracy')\n",
    "        \n",
    "        plt.ylabel('score')\n",
    "        plt.xlabel('step')\n",
    "        plt.legend(loc=acc_legend_loc)\n",
    "    # 将绘制结果保存\n",
    "    plt.savefig(fig_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ddd02",
   "metadata": {},
   "source": [
    "## 随机梯度下降法\n",
    "\n",
    "三种随梯度下降法的区别在于批量大小的不同。\n",
    "第一个实验，我们观察批量大小对梯度下降法的影响。具体来说，我们重新训练第四章的鸢尾花识别任务，使用不同的批量大小，看看会对结果产生什么影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8254c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:08.999803400Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.fc1(inputs)\n",
    "        outputs = self.act(outputs)\n",
    "        outputs = self.fc2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41396432",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:09.000802300Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def load_data(shuffle=True):\n",
    "    x = torch.tensor(load_iris().data)\n",
    "    y = torch.tensor(load_iris().target)\n",
    "    \n",
    "    # 数据归一化\n",
    "    x_min = torch.min(x, dim=0).values\n",
    "    x_max = torch.max(x, dim=0).values\n",
    "    x = (x-x_min)/(x_max-x_min)\n",
    "    \n",
    "    if shuffle:\n",
    "        idx = torch.randperm(x.shape[0])\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, mode='train', num_train=120, num_dev=15):\n",
    "        super(IrisDataset,self).__init__()\n",
    "        x, y = load_data(shuffle=True)\n",
    "        if mode == 'train':\n",
    "            self.x, self.y = x[:num_train], y[:num_train]\n",
    "        elif mode == 'dev':\n",
    "            self.x, self.y = x[num_train:num_train + num_dev], y[num_train:num_train + num_dev]\n",
    "        else:\n",
    "            self.x, self.y = x[num_train + num_dev:], y[num_train + num_dev:]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f745c98",
   "metadata": {},
   "source": [
    "测试不同批量大小时,需要取消注释相应的行(batch_size).若在notebook中运行时,修改代码后需要重新运行单元格."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93559761",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:09.001801900Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "batch_size = 24\n",
    "# batch_size = 120 \n",
    "\n",
    "# 分别构建训练集、验证集和测试集\n",
    "train_dataset = IrisDataset(mode='train')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b23c8",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "请注意: 修改批量大小后,记得修改绘制图像保存的文件名后再运行下面的单元格.\\\n",
    "另一种解决办法是实例化多个Runner类进行不同批量大小的实验,并对不同的Runner实例绘图,但这需要同样实例化多个train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ed79b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:09.003800500Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "output_size = 3\n",
    "hidden_size = 6\n",
    "# 定义模型\n",
    "model = FeedForward(input_size, hidden_size, output_size)\n",
    "# 定义损失函数\n",
    "loss_fn = F.cross_entropy\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "# 实例化辅助runner类\n",
    "runner = Runner(model, optimizer, loss_fn)\n",
    "# 模型训练\n",
    "runner.train(train_loader, num_epochs=50, log_steps=10)\n",
    "plot_training_loss_acc(runner, 'chapter7_24batchsize.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5fa28",
   "metadata": {},
   "source": [
    "## 动量法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f23e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:09.004803100Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_loss(net, data_iter, loss):\n",
    "    \"\"\"评估给定数据集上模型的损失\n",
    "\n",
    "    Defined in :numref:`sec_model_selection`\"\"\"\n",
    "    metric = d2l.Accumulator(2)  # 损失的总和,样本数量\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(torch.float32)\n",
    "        out = net(X)\n",
    "#         y = d2l.reshape(y, out.shape)\n",
    "        l = loss(out, y.long())\n",
    "        metric.add(d2l.reduce_sum(l), d2l.size(l))\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2):\n",
    "    \"\"\"Defined in :numref:`sec_minibatches`\"\"\"\n",
    "    # 初始化模型\n",
    "    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 3),\n",
    "                     requires_grad=True)\n",
    "    b = torch.zeros((3), requires_grad=True)\n",
    "    # 训练模型\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.9, 1.1])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    \n",
    "    # 这是一个单层线性层     \n",
    "    net = lambda X: d2l.linreg(X, w, b)\n",
    "    loss = F.cross_entropy \n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(torch.float32)\n",
    "            l = loss(net(X), y.long()).mean()\n",
    "            l.backward()\n",
    "            trainer_fn([w, b], states, hyperparams)\n",
    "            n += X.shape[0]\n",
    "            if n % 48 == 0:\n",
    "                timer.stop()\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (evaluate_loss(net, data_iter, loss),))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')\n",
    "\n",
    "    return timer.cumsum(), animator.Y[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fb166",
   "metadata": {},
   "source": [
    "接下来，我们实现一个可以满足上面线性层的，带有动量的随机梯度下降法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6246db2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:09.005805200Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_momentum_states(feature_dim):\n",
    "    v_w = torch.zeros((feature_dim, 3))\n",
    "    v_b = torch.zeros(3)\n",
    "    return (v_w, v_b)\n",
    "\n",
    "def sgd_momentum(params, states, hyperparams):\n",
    "    for p, v in zip(params, states):\n",
    "        with torch.no_grad():\n",
    "            v[:] = hyperparams['momentum'] * v + p.grad\n",
    "            p[:] -= hyperparams['lr'] * v\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "在训练阶段,请将批量大小为24"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8435047032ff024f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfbb07e",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:09.007801900Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "train(sgd_momentum, init_momentum_states(4),{'lr':lr, 'momentum':momentum}, train_loader, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cfc27",
   "metadata": {},
   "source": [
    "## 简洁实现\n",
    "\n",
    "深度学习框架中的优化器早已构建了动量法.\\\n",
    "我们实例化一个sgd类, 并将超参数设置为对应的形式即可."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc3857",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T08:44:09.008801300Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "output_size = 3\n",
    "hidden_size = 6\n",
    "# 定义模型\n",
    "model = FeedForward(input_size, hidden_size, output_size)\n",
    "# 定义损失函数\n",
    "loss_fn = F.cross_entropy\n",
    "# 定义优化器, 与之前的实验不同的是,momentum参数被修改\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
    "# 实例化辅助runner类\n",
    "runner = Runner(model, optimizer, loss_fn)\n",
    "# 模型训练\n",
    "runner.train(train_loader, num_epochs=50, log_steps=10)\n",
    "plot_training_loss_acc(runner, 'chapter7_24batchsize_momentum.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的工具包\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实践\n",
    "\n",
    "接下来，我们要借助深度学习框架PyTorch实现一个残差网络，并基于此完成图像分类任务。\\\n",
    "残差网络是典型的卷积神经网络，由许多卷积层和汇聚层堆叠而成；其独特的思想在于，将卷积神经网络的输入与输出相加，即残差连接。所以，在实现残差网络时，需要注意的保证输入和输出的维度（宽，高和通道）是相同的，零填充和1×1卷积会帮我们完成这些任务！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  残差连接, 输入和输出的维度有时是相同的, 有时是不同的, 所以需要 use_1x1conv来判断是否需要 \n",
    "class Residual(nn.Module):  \n",
    "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        # 批量归一化层，将会在第7章讲到    \n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "# 残差网络是由几个不同的残差块组成的\n",
    "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__ (self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        \n",
    "        self.b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "        self.b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "        self.b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "        self.b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "        self.head = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        net = nn.Sequential(self.b1, self.b2, self.b3, self.b4, self.b5, self.head)\n",
    "        \n",
    "        return net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在, 让我们构建用于图片分类任务的训练数据集。我们使用CIFAR-10数据集，该数据集包含10个种类、共60000幅图像，我们仅使用1000张图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_csv_labels(fname):\n",
    "    \"\"\"读取fname来给标签字典返回一个文件名\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # 跳过文件头行(列名)\n",
    "        lines = f.readlines()[1:]\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    return dict(((name, label) for name, label in tokens))\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, folder_path, fname):\n",
    "        self.labels = read_csv_labels(os.path.join(folder_path,fname))\n",
    "        self.folder_path = os.path.join(folder_path, 'train')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(self.folder_path + '/' + str(idx+1) + '.png')\n",
    "        label = self.labels[str(idx+1)]\n",
    "        \n",
    "        return img, torch.tensor(int(label))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化CIFAR10数据集，并指定批量大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_data = CIFAR10Dataset('cifar10_tiny', 'trainLabels.csv')\n",
    "train_iter = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Runner(object):\n",
    "    def __init__(self, model, optimizer, loss_fn, metric=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        # 用于计算评价指标\n",
    "        self.metric = metric\n",
    "        \n",
    "        # 记录训练过程中的评价指标变化\n",
    "        self.dev_scores = []\n",
    "        # 记录训练过程中的损失变化\n",
    "        self.train_epoch_losses = []\n",
    "        self.dev_losses = []\n",
    "        # 记录全局最优评价指标\n",
    "        self.best_score = 0\n",
    "   \n",
    " \n",
    "# 模型训练阶段\n",
    "    def train(self, train_loader, dev_loader=None, **kwargs):\n",
    "        # 将模型设置为训练模式，此时模型的参数会被更新\n",
    "        self.model.train()\n",
    "        \n",
    "        num_epochs = kwargs.get('num_epochs', 0)\n",
    "        log_steps = kwargs.get('log_steps', 100)\n",
    "        save_path = kwargs.get('save_path','best_model.pth')\n",
    "        eval_steps = kwargs.get('eval_steps', 0)\n",
    "        # 运行的step数，不等于epoch数\n",
    "        global_step = 0\n",
    "        \n",
    "        if eval_steps:\n",
    "            if dev_loader is None:\n",
    "                raise RuntimeError('Error: dev_loader can not be None!')\n",
    "            if self.metric is None:\n",
    "                raise RuntimeError('Error: Metric can not be None')\n",
    "                \n",
    "        # 遍历训练的轮数\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            # 遍历数据集\n",
    "            for step, data in enumerate(train_loader):\n",
    "                x, y = data\n",
    "                logits = self.model(x.float())\n",
    "                loss = self.loss_fn(logits, y.long())\n",
    "                total_loss += loss\n",
    "                if step%log_steps == 0:\n",
    "                    print(f'loss:{loss.item():.5f}')\n",
    "                    \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            # 每隔一定轮次进行一次验证，由eval_steps参数控制，可以采用不同的验证判断条件\n",
    "            if eval_steps != 0 :\n",
    "                if (epoch+1) % eval_steps ==  0:\n",
    "\n",
    "                    dev_score, dev_loss = self.evaluate(dev_loader, global_step=global_step)\n",
    "                    print(f'[Evalute] dev score:{dev_score:.5f}, dev loss:{dev_loss:.5f}')\n",
    "                \n",
    "                    if dev_score > self.best_score:\n",
    "                        self.save_model(f'model_{epoch+1}.pth')\n",
    "                    \n",
    "                        print(f'[Evaluate]best accuracy performance has been updated: {self.best_score:.5f}-->{dev_score:.5f}')\n",
    "                        self.best_score = dev_score\n",
    "                    \n",
    "                # 验证过程结束后，请记住将模型调回训练模式   \n",
    "                    self.model.train()\n",
    "            \n",
    "            global_step += 1\n",
    "            # 保存当前轮次训练损失的累计值\n",
    "            train_loss = (total_loss/len(train_loader)).item()\n",
    "            self.train_epoch_losses.append((global_step,train_loss))\n",
    "        self.save_model(f'{save_path}.pth')   \n",
    "        print('[Train] Train done')\n",
    "        \n",
    "    # 模型评价阶段\n",
    "    def evaluate(self, dev_loader, **kwargs):\n",
    "        assert self.metric is not None\n",
    "        # 将模型设置为验证模式，此模式下，模型的参数不会更新\n",
    "        self.model.eval()\n",
    "        global_step = kwargs.get('global_step',-1)\n",
    "        total_loss = 0\n",
    "        self.metric.reset()\n",
    "        \n",
    "        for batch_id, data in enumerate(dev_loader):\n",
    "            x, y = data\n",
    "            logits = self.model(x.float())\n",
    "            loss = self.loss_fn(logits, y.long()).item()\n",
    "            total_loss += loss \n",
    "            self.metric.update(logits, y)\n",
    "            \n",
    "        dev_loss = (total_loss/len(dev_loader))\n",
    "        self.dev_losses.append((global_step, dev_loss))\n",
    "        dev_score = self.metric.accumulate()\n",
    "        self.dev_scores.append(dev_score)\n",
    "        return dev_score, dev_loss\n",
    "    \n",
    "    # 模型预测阶段，\n",
    "    def predict(self, x, **kwargs):\n",
    "        self.model.eval()\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "    \n",
    "    # 保存模型的参数\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        \n",
    "    # 读取模型的参数\n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.30708\n",
      "loss:3.13586\n",
      "loss:2.84148\n",
      "loss:2.90006\n",
      "loss:2.60240\n",
      "loss:2.37752\n",
      "loss:1.88666\n",
      "loss:2.02436\n",
      "loss:1.90529\n",
      "loss:1.76597\n",
      "[Train] Train done\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "# 定义模型\n",
    "model = ResNet(num_classes)\n",
    "# 定义损失函数\n",
    "loss_fn = F.cross_entropy\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "runner = Runner(model, optimizer, loss_fn, metric=None)\n",
    "runner.train(train_iter, num_epochs=10, save_path='chapter_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测\n",
    "    \n",
    "   模型预测的方法与上一章完全相同，请大家自己尝试。\n",
    "   在Cifar-10中，十个类的对应关系为 0: 飞机、1: 汽车、2: 鸟类、3: 猫、4: 鹿、5: 狗、6: 青蛙、7: 马、8: 船、9:卡车"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "   与之前的模型有所不同，循环神经网络引入了隐藏状态和时间步两个新概念。当前时间步的隐藏状态由当前时间的输入与上一个时间步的隐藏状态一起计算出。\n",
    "   根据隐藏状态的计算公式，需要计算两次矩阵乘法和三次加法才能得到当前时刻的隐藏状态。我们通过代码说明: 该计算公式等价于将当前时刻的输入与上一个时间步的隐藏状态做拼接，将两个权重矩阵做拼接，然后对两个拼接后的结果做矩阵乘法。此处展示我们省略了偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3222,  2.6943,  1.1887,  0.6252],\n",
       "        [-0.8495, -0.1767,  0.0372, -0.2811],\n",
       "        [ 8.3495, -2.9154, -0.2664,  2.7573]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X为模拟的输入，H为模拟的隐藏状态，在实际情况时要更复杂一些\n",
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是按照公式计算得到的结果，下面是拼接后计算得到的结果，可以看出两个结果完全相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3222,  2.6943,  1.1887,  0.6252],\n",
       "        [-0.8495, -0.1767,  0.0372, -0.2811],\n",
       "        [ 8.3495, -2.9154, -0.2664,  2.7573]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们实现一个简单的循环神经网络的例子。\n",
    "从下面的函数中，我们可以看到，在循环神经网络中，需要遍历时间步，并保存每一步都输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H = state\n",
    "    outputs = []\n",
    "    # X的形状：(批量大小，词表大小)\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[23.5445, 29.6079, 23.8511,  ..., 27.4512, 22.2955, 26.7673],\n",
      "        [23.6436, 29.7690, 23.3254,  ..., 27.7294, 22.4751, 27.1660],\n",
      "        [23.6796, 29.2678, 23.2866,  ..., 27.4755, 21.6524, 27.6663],\n",
      "        ...,\n",
      "        [23.5445, 29.6079, 23.8511,  ..., 27.4512, 22.2955, 26.7673],\n",
      "        [23.6436, 29.7690, 23.3254,  ..., 27.7294, 22.4751, 27.1660],\n",
      "        [23.6796, 29.2678, 23.2866,  ..., 27.4755, 21.6524, 27.6663]]), (tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),))\n"
     ]
    }
   ],
   "source": [
    "inputs=torch.rand(10,3,50)\n",
    "params=[torch.rand((50,50)),torch.rand((50,50)),torch.rand((3,50)),torch.rand((50,60)),torch.rand((3,60))]\n",
    "state=torch.rand((3,50))\n",
    "output=rnn(inputs,state,params)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   在循环神经网络的训练中，当时间步较大时，可能导致数值不稳定， 例如梯度爆炸或梯度消失，所以一个很重要的步骤是梯度裁剪。通过下面的函数，梯度范数永远不会超过给定的阈值， 并且更新后的梯度完全与的原始方向对齐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta): \n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用pytorch构建简单的RNN网络并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义RNN模型\n",
    "class Rnn(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Rnn, self).__init__()\n",
    "        # 定义RNN网络\n",
    "        ## hidden_size是自己设置的，取值都是32,64,128这样来取值\n",
    "        ## num_layers是隐藏层数量，超过2层那就是深度循环神经网络了\n",
    "        self.rnn = nn.RNN(\n",
    "                input_size=input_size,\n",
    "                hidden_size=32,\n",
    "                num_layers=1,\n",
    "                batch_first=True  # 输入形状为[批量大小, 数据序列长度, 特征维度]\n",
    "                )\n",
    "        # 定义全连接层\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "    # 定义前向传播函数\n",
    "    def forward(self, x, h_0):\n",
    "        r_out, h_n = self.rnn(x, h_0)\n",
    "        # print(\"数据输出结果；隐藏层数据结果\", r_out, h_n)\n",
    "        # print(\"r_out.size()， h_n.size()\", r_out.size(), h_n.size())\n",
    "        outs = []\n",
    "        # r_out.size=[1,10,32]即将一个长度为10的序列的每个元素都映射到隐藏层上\n",
    "        for time in range(r_out.size(1)):  \n",
    "            # print(\"映射\", r_out[:, time, :])\n",
    "            # 依次抽取序列中每个单词,将之通过全连接层并输出.r_out[:, 0, :].size()=[1,32] -> [1,1]\n",
    "            outs.append(self.out(r_out[:, time, :])) \n",
    "            # print(\"outs\", outs)\n",
    "        # stack函数在dim=1上叠加:10*[1,1] -> [1,10,1] 同时h_n已经被更新\n",
    "        return torch.stack(outs, dim=1), h_n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnn(\n",
      "  (rnn): RNN(1, 32, batch_first=True)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "TIME_STEP = 10\n",
    "INPUT_SIZE = 1\n",
    "LR = 0.02\n",
    "model = Rnn(INPUT_SIZE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此处使用的是均方误差损失\n",
    "import numpy as np\n",
    "loss_func = nn.MSELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "h_state = None  # 初始化h_state为None\n",
    "\n",
    "for step in range(300):\n",
    "    # 人工生成输入和输出,输入x.size=[1,10,1],输出y.size=[1,10,1]\n",
    "    start, end = step * np.pi, (step + 1)*np.pi\n",
    "    # np.linspace生成一个指定大小，指定数据区间的均匀分布序列，TIME_STEP是生成数量\n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32) \n",
    "    # print(\"steps\", steps)\n",
    "    x_np = np.sin(steps)\n",
    "    y_np = np.cos(steps)\n",
    "    # print(\"x_np,y_np\", x_np, y_np)\n",
    "    # 从numpy.ndarray创建一个张量 np.newaxis增加新的维度\n",
    "    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])\n",
    "    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])\n",
    "    # print(\"x,y\", x,y)\n",
    "\n",
    "    # 将x通过网络,长度为10的序列通过网络得到最终隐藏层状态h_state和长度为10的输出prediction:[1,10,1]\n",
    "    prediction, h_state = model(x, h_state)\n",
    "    h_state = h_state.data  \n",
    "    # 这一步只取了h_state.data.因为h_state包含.data和.grad 舍弃了梯度\n",
    "    # print(\"precision, h_state.data\", prediction, h_state)\n",
    "    # print(\"prediction.size(), h_state.size()\", prediction.size(), h_state.size())\n",
    "    \n",
    "    # 反向传播\n",
    "    loss = loss_func(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 更新优化器参数\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 对最后一次的结果作图查看网络的预测效果\n",
    "plt.plot(steps, y_np.flatten(), 'r-')\n",
    "plt.plot(steps, prediction.data.numpy().flatten(), 'b-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9ccd8639d7ac6d8ae46f08631d02de0d1c9f4a08850208985333be71082afd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

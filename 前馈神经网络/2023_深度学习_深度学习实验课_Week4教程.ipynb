{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前馈神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的工具包\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# 绘画时使用的工具包\n",
    "import matplotlib.pyplot as plt\n",
    "# 当在notebook中运行时，请键入以下代码，图片可以在单元格中输出，在编译器中运行时不需要\n",
    "%matplotlib inline\n",
    "# 导入鸢尾花数据集\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 净活性值\n",
    "\n",
    "假设一个神经元接收 𝐷 个输入𝑥1 , 𝑥2 , ⋯ , 𝑥𝐷，令向量𝒙 = [𝑥1 ; 𝑥2 ; ⋯ ; 𝑥𝐷]来表示这组输入，并用净活性值 𝑧 表示一个神经元所获得的输入信号𝒙的加权和。\n",
    "为了提高效率，通常会将N  (N>1) 个样本归为一组进行成批预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x 表示两个含有5个特征的样本，x是一个二维的tensor\n",
    "x = torch.randn((2,5))\n",
    "# w 表示含有5个参数的权重向量，w是一个二维的tensor\n",
    "w = torch.randn((5,1))\n",
    "# 偏置项，b是一个二维的tensor，但b只有一个数值\n",
    "b = torch.randn((1,1))\n",
    "# 矩阵乘法，请注意 x 和 w 的顺序，与 b 相加时使用了广播机制\n",
    "z = torch.matmul(x, w) + b\n",
    "# 另一种写法\n",
    "z_2 = x@w + b \n",
    "# 打印结果，z是一个二维的tensor，表示两个样本经过神经元后的各自净活性值\n",
    "print('output z:' , z)\n",
    "print('shape of z: ', z.shape)\n",
    "print('output z_2:', z_2)\n",
    "print('shape of z:', z_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简洁实现，使用PyTorch提供的类。请牢记参数的实际意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个线性层，接受输入维度是5，输出维度是1\n",
    "net = nn.Linear(5,1)\n",
    "z_3 = net(x)\n",
    "# 打印结果，z2的形状与z一样，含义也与z一样\n",
    "print('output z2: ', z_3)\n",
    "print('shape of z2:' , z_3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数\n",
    "\n",
    "净活性值在经过一个非线性函数𝑓(⋅)后，得到神经元的活性值𝑎，也就是该层神经元的输出。\n",
    "\n",
    "激活函数通常为非线性函数，常用的有Sigmoid型激活函数和ReLU激活函数，要想实现他们，其实非常的简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic 函数\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + torch.exp(-z))\n",
    "# Tanh函数\n",
    "def tanh(z):\n",
    "    return (torch.exp(z) - torch.exp(-z)) / (torch.exp(z) + torch.exp(-z))\n",
    "# ReLU函数\n",
    "def relu(z):\n",
    "    return torch.max(z, torch.zeros_like(z))\n",
    "# leakyReLU函数\n",
    "def leaky_relu(z, gamma=0.1):\n",
    "    positive = torch.max(z, torch.zeros_like(z))\n",
    "    negative = torch.min(z, torch.zeros_like(z))\n",
    "    return positive + gamma * negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出激活函数的图像\n",
    "# 从-10 到 10 每间隔0.01 取一个数\n",
    "a = torch.arange(-10, 10, 0.01)\n",
    "plt.figure()\n",
    "# 在第一个子图中绘制Sigmoid型激活函数\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(a.tolist(), logistic(a).tolist(), color= 'red', label='logistic')\n",
    "plt.plot(a.tolist(), tanh(a).tolist(), color='blue', linestyle='--', label='tanh')\n",
    "# 在第二个子图中绘制ReLU型激活函数\n",
    "plt.subplot(222)\n",
    "plt.plot(a.tolist(), relu(a).tolist(), color='g', label='relu')\n",
    "plt.plot(a.tolist(), leaky_relu(a).tolist(), color='black',linestyle='--', label='leaky relu')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简洁实现，使用PyTorch提供的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z为前面计算的净活性值\n",
    "sig_output = torch.sigmoid(z)\n",
    "tan_output = torch.tanh(z)\n",
    "relu_output = torch.relu(z)\n",
    "# 打印输出结果\n",
    "print('sigmoid:',sig_output)\n",
    "print('tanh:', tan_output)\n",
    "print('ReLU:', relu_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于前馈神经网络的二分类任务\n",
    "\n",
    "想要完成一个深度学习的任务,可以简单地将其分为7步，构建数据集、构建模型、设计损失函数、根据损失函数模型训练阶段、训练同时的测试阶段、训练结果展示、预测阶段；其中前4步是不可缺少的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性层算子，请一定注意继承自 nn. Module, 这会帮你解决许多细节上的问题\n",
    "class Linear(nn.Module): \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Linear, self).__init__()\n",
    "        self.params = {}\n",
    "        self.params['W'] = nn.Parameter(torch.randn(input_size, output_size, requires_grad=True))\n",
    "        self.params['b'] = nn.Parameter(torch.randn(1, output_size, requires_grad=True))\n",
    "        self.grads = {}\n",
    "        self.inputs = None\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        outputs = torch.matmul(inputs, self.params['W']) + self.params['b']\n",
    "        return outputs\n",
    "    \n",
    "# 实现一个两层的前馈神经网络\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super( MLP, self).__init__()\n",
    "        self.fc1 = Linear(input_size, hidden_size)\n",
    "        self.fc2 = Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.fc1(x)\n",
    "        a1 = logistic(z1)\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = logistic(z2)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones((1,10))\n",
    "input_size, hidden_size, output_size = 10, 5, 2\n",
    "net = MLP(input_size, hidden_size, output_size)\n",
    "output = net(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数，对于分类问题，常使用交叉熵作为损失函数\n",
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat[range(len(y_hat)), y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请深刻理解这种表达形式。\n",
    "# y的含义是，第一个样本是第1类，第二个样本是第3类；\n",
    "# y_hat的含义是，模型预测第一个样本属于不同类的置信度分别是0.1、0.3和0.6，模型预测第二个样本属于不同类别的置信度分别是0.3、0.2、0.5\n",
    "\n",
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播算法\n",
    "\n",
    "截止到目前，你应该对神经网络的前向过程有了一定的理解，但我们实现的激活函数和损失函数还都不支持反向传播。\n",
    "接下来，让我们加入这个功能。神经网络相当于一个复合函数，需要利用链式法则进行反向传播来计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss():\n",
    "    def __init__(self, model):\n",
    "        self.predicts = None\n",
    "        self.labels = None\n",
    "        self.num = None\n",
    "        self.model = model\n",
    "        \n",
    "    def __call__(self, predicts, labels):\n",
    "        return self.forward(predicts, labels)\n",
    "    \n",
    "    def forward(self, predicts, labels):\n",
    "        self.predicts = predicts\n",
    "        self.labels = labels\n",
    "        self.num = self.predicts.shape[0]\n",
    "        loss = - torch.log(y_hat[range(len(y_hat)), y])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        inputs_grads = -1 * (self.labels/self.predicts - (1 - self.labels)/(1 - self.predicts))/self.num\n",
    "        \n",
    "        self.model.backward(inputs_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic():\n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "        self.params = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs =  1.0 / (1.0 + torch.exp(-inputs))\n",
    "        self.outputs = outputs\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, outputs_grads=None):\n",
    "        if outputs_grads is None:\n",
    "            outputs_grads = torch.ones(self.outputs.shape)\n",
    "        outputs_grad_inputs = torch.multiply(self.outputs, (1.0 - self.outputs))\n",
    "        return torch.multiply(outputs_grads, outputs_grad_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = Logistic()\n",
    "x = torch.tensor([3,3,4,2])\n",
    "y = act(x)\n",
    "\n",
    "z = act.backward()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.params = {}\n",
    "        self.params['W'] = nn.Parameter(torch.randn(input_size, output_size, requires_grad=True))\n",
    "        self.params['b'] = nn.Parameter(torch.randn(1, output_size, requires_grad=True))\n",
    "        self.inputs = None\n",
    "        self.grads = {}\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        outputs = torch.matmul(self.inputs, self.params['W']) + self.params['b']\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, grads=None):\n",
    "        if grads == None:\n",
    "            grads = torch.ones(self.params['W'].shape)\n",
    "        self.grads['w'] = torch.matmul(self.inputs.T, grads)\n",
    "        self.grads['b'] = torch.sum(grads, dim=0)\n",
    "        return torch.matmul(grads, self.params['W'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Linear(4, 2)\n",
    "x = torch.tensor([1,1,1,1], dtype=torch.float32)\n",
    "y = net(x)\n",
    "z = net.backward()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导\n",
    "\n",
    "PyTorch中，所有神经网络的核心是 autograd 包。autograd包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义 ( define-by-run ）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的。\n",
    "\n",
    "每个张量都有一个.grad_fn属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是 None )。下面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单的情况，X是一个标量\n",
    "x = torch.tensor(2, dtype=torch.float32, requires_grad=True)\n",
    "y = x ** 2 + 4 * x \n",
    "print(x.grad)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个gradient参数，该参数是形状匹配的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "# y是一个矩阵\n",
    "y = x ** 2 + 4 * x\n",
    "y.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = x ** 3 + 2 * x\n",
    "# z是一个标量\n",
    "z = u.sum()\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实践\n",
    "\n",
    "   在之前实现的代码中，借助深度学习框架的帮助，我们已经完成了大部分功能。在最后的实验中，我们引入小批量和随机梯度下降法，即每次仅根据数据集一个批量大小的数据来更新参数。\n",
    "   为了使用小批量，通常的做法是构建一个数据迭代器,在每次迭代时从全部数据集中获取指定数量的数据。我们可以借助深度学习框架中的Dataset类和DataLoader类来实现此功能。\n",
    "   随机梯度下降法则完全引用深度学习框架已经实现好的方法。\n",
    "   在继承Dataset类时，应实现\\_\\_getitem\\_\\_和\\_\\_len\\_\\_两个方法，前者会根据索引获取数据集中指定的样本，后者则返回数据集样本的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此函数用于加载鸢尾花数据集\n",
    "def load_data(shuffle=True):\n",
    "    x = torch.tensor(load_iris().data)\n",
    "    y = torch.tensor(load_iris().target)\n",
    "    \n",
    "    # 数据归一化\n",
    "    x_min = torch.min(x, dim=0).values\n",
    "    x_max = torch.max(x, dim=0).values\n",
    "    x = (x-x_min)/(x_max-x_min)\n",
    "    \n",
    "    if shuffle:\n",
    "        idx = torch.randperm(x.shape[0])\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建自己的数据集,继承自Dataset类\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, mode='train', num_train=120, num_dev=15):\n",
    "        super(IrisDataset,self).__init__()\n",
    "        x, y = load_data(shuffle=True)\n",
    "        if mode == 'train':\n",
    "            self.x, self.y = x[:num_train], y[:num_train]\n",
    "        elif mode == 'dev':\n",
    "            self.x, self.y = x[num_train:num_train + num_dev], y[num_train:num_train + num_dev]\n",
    "        else:\n",
    "            self.x, self.y = x[num_train + num_dev:], y[num_train + num_dev:]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化数据集后,使用DataLoader进行封装.为简便可使数据集样本总数为批量大小的整数倍,但实际并不强制要求;shuffle参数的含义是是否随机从数据集中取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# 分别构建训练集、验证集和测试集\n",
    "train_dataset = IrisDataset(mode='train')\n",
    "dev_dataset = IrisDataset(mode='dev')\n",
    "test_dataset = IrisDataset(mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完全依靠深度学习框架，构建一个简单的两层前馈神经网络，输入层神经元个数为4，输出层神经元个数为3，隐含层神经元个数为6。\n",
    "这个前馈神经网络和我们在前面实现的MLP类最大的区别在于，我们实现类中使用了自己写的激活函数，该激活函数不能通过反向传播更新参数，但深度学习框架已经帮我们完成了这个功能。（其实通过简单的改动，我们的激活函数也可以反传梯度，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.fc1(inputs)\n",
    "        outputs = self.act(outputs)\n",
    "        outputs = self.fc2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "现在，让我们实现第一个辅助功能——计算预测的准确率。Accuracy支持对每一个回合中每批数据进行评价，并将结果累积，最终获得整批数据的评价结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 支持分批进行模型评价的 Accuracy 类\n",
    "class Accuracy:\n",
    "    def __init__(self, is_logist=True):\n",
    "        # 正确样本个数\n",
    "        self.num_correct = 0\n",
    "        # 样本总数\n",
    "        self.num_count = 0\n",
    "        self.is_logist = is_logist\n",
    "    \n",
    "    def update(self, outputs, labels):\n",
    "        # 判断是否为二分类任务\n",
    "        if outputs.shape[1] == 1:\n",
    "            outputs = outputs.squeeze(-1)\n",
    "            # 判断是否是logit形式的预测值\n",
    "            if self.is_logist:\n",
    "                preds = (outputs >= 0).long()\n",
    "            else:\n",
    "                preds = (preds >= 0.5).long()\n",
    "        else:\n",
    "            # 多分类任务时，计算最大元素索引作为类别\n",
    "            preds = torch.argmax(outputs, dim=1).long()\n",
    "            \n",
    "        # 获取本批数据中预测正确的样本个数\n",
    "        labels = labels.squeeze(-1)\n",
    "        batch_correct = (preds==labels).float().sum()\n",
    "        batch_count = len(labels)\n",
    "        # 更新\n",
    "        self.num_correct += batch_correct\n",
    "        self.num_count += batch_count\n",
    "        \n",
    "    def accumulate(self):\n",
    "        # 使用累计的数据，计算总的评价指标\n",
    "        if self.num_count == 0:\n",
    "            return 0\n",
    "        return self.num_correct/self.num_count\n",
    "    \n",
    "    def reset(self):\n",
    "        self.num_correct = 0\n",
    "        self.num_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用之前构造的数据进行测试\n",
    "acc = Accuracy()\n",
    "acc.update(y_hat, y)\n",
    "acc.num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "现在，让我们实现第二个辅助功能——一个整合了训练过程的类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(object):\n",
    "    def __init__(self, model, optimizer, loss_fn, metric, **kwargs):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        # 用于计算评价指标\n",
    "        self.metric = metric\n",
    "        \n",
    "        # 记录训练过程中的评价指标变化\n",
    "        self.dev_scores = []\n",
    "        # 记录训练过程中的损失变化\n",
    "        self.train_epoch_losses = []\n",
    "        self.dev_losses = []\n",
    "        # 记录全局最优评价指标\n",
    "        self.best_score = 0\n",
    "   \n",
    " \n",
    "# 模型训练阶段\n",
    "    def train(self, train_loader, dev_loader=None, **kwargs):\n",
    "        # 将模型设置为训练模式，此时模型的参数会被更新\n",
    "        self.model.train()\n",
    "        \n",
    "        num_epochs = kwargs.get('num_epochs', 0)\n",
    "        log_steps = kwargs.get('log_steps', 100)\n",
    "        save_path = kwargs.get('save_path','best_mode.pth')\n",
    "        eval_steps = kwargs.get('eval_steps', 0)\n",
    "        # 运行的step数，不等于epoch数\n",
    "        global_step = 0\n",
    "        \n",
    "        if eval_steps:\n",
    "            if dev_loader is None:\n",
    "                raise RuntimeError('Error: dev_loader can not be None!')\n",
    "            if self.metric is None:\n",
    "                raise RuntimeError('Error: Metric can not be None')\n",
    "                \n",
    "        # 遍历训练的轮数\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            # 遍历数据集\n",
    "            for step, data in enumerate(train_loader):\n",
    "                x, y = data\n",
    "                logits = self.model(x.float())\n",
    "                loss = self.loss_fn(logits, y.long())\n",
    "                total_loss += loss\n",
    "                if log_steps and global_step%log_steps == 0:\n",
    "                    print(f'loss:{loss.item():.5f}')\n",
    "                    \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            # 每隔一定轮次进行一次验证，由eval_steps参数控制，可以采用不同的验证判断条件\n",
    "            if (epoch+1)% eval_steps ==  0:\n",
    "\n",
    "                dev_score, dev_loss = self.evaluate(dev_loader, global_step=global_step)\n",
    "                print(f'[Evalute] dev score:{dev_score:.5f}, dev loss:{dev_loss:.5f}')\n",
    "                \n",
    "                if dev_score > self.best_score:\n",
    "                    self.save_model(f'model_{epoch+1}.pth')\n",
    "                    \n",
    "                    print(f'[Evaluate]best accuracy performance has been updated: {self.best_score:.5f}-->{dev_score:.5f}')\n",
    "                    self.best_score = dev_score\n",
    "                    \n",
    "                # 验证过程结束后，请记住将模型调回训练模式   \n",
    "                self.model.train()\n",
    "            \n",
    "            global_step += 1\n",
    "            # 保存当前轮次训练损失的累计值\n",
    "            train_loss = (total_loss/len(train_loader)).item()\n",
    "            self.train_epoch_losses.append((global_step,train_loss))\n",
    "            \n",
    "        print('[Train] Train done')\n",
    "        \n",
    "    # 模型评价阶段\n",
    "    def evaluate(self, dev_loader, **kwargs):\n",
    "        assert self.metric is not None\n",
    "        # 将模型设置为验证模式，此模式下，模型的参数不会更新\n",
    "        self.model.eval()\n",
    "        global_step = kwargs.get('global_step',-1)\n",
    "        total_loss = 0\n",
    "        self.metric.reset()\n",
    "        \n",
    "        for batch_id, data in enumerate(dev_loader):\n",
    "            x, y = data\n",
    "            logits = self.model(x.float())\n",
    "            loss = self.loss_fn(logits, y.long()).item()\n",
    "            total_loss += loss \n",
    "            self.metric.update(logits, y)\n",
    "            \n",
    "        dev_loss = (total_loss/len(dev_loader))\n",
    "        self.dev_losses.append((global_step, dev_loss))\n",
    "        dev_score = self.metric.accumulate()\n",
    "        self.dev_scores.append(dev_score)\n",
    "        return dev_score, dev_loss\n",
    "    \n",
    "    # 模型预测阶段，\n",
    "    def predict(self, x, **kwargs):\n",
    "        self.model.eval()\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "    \n",
    "    # 保存模型的参数\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.model.state_dict(),save_path)\n",
    "        \n",
    "    # 读取模型的参数\n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "output_size = 3\n",
    "hidden_size = 6\n",
    "# 定义模型\n",
    "model = FeedForward(input_size, hidden_size, output_size)\n",
    "# 定义损失函数\n",
    "loss_fn = F.cross_entropy\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "# 定义评价方法\n",
    "metric = Accuracy(is_logist=True)\n",
    "# 实例化辅助runner类\n",
    "runner = Runner(model, optimizer, loss_fn, metric)\n",
    "# 模型训练\n",
    "runner.train(train_loader, dev_loader, num_epochs=50, log_steps=10, eval_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中，Runner类记录了每次迭代后的损失函数值和准确率(在不同设置下，记录的频率会有所不同)，可视化函数利用这些记录，绘制图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练集和验证集的损失变化以及验证集上的准确率变化曲线\n",
    "def plot_training_loss_acc(runner, fig_name, fig_size=(16, 6), sample_step=20, loss_legend_loc='upper right', acc_legend_loc='lower right',\n",
    "                          train_color = '#8E004D', dev_color = '#E20079', fontsize='x-large', train_linestyle='-', dev_linestyle='--'):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.subplot(1,2,1)\n",
    "    train_items = runner.train_epoch_losses[::sample_step]\n",
    "    train_steps = [x[0] for x in train_items]\n",
    "    train_losses = [x[1] for x in train_items]\n",
    "    \n",
    "    plt.plot(train_steps, train_losses, color=train_color, linestyle=train_linestyle, label='Train loss')\n",
    "    if len(runner.dev_losses) > 0:\n",
    "        dev_steps = [x[0] for x in runner.dev_losses]\n",
    "        dev_losses = [x[1] for x in runner.dev_losses]\n",
    "        plt.plot(dev_steps, dev_losses, color=dev_color, linestyle=dev_linestyle,label='dev loss')\n",
    "    \n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('step')\n",
    "    plt.legend(loc=loss_legend_loc)\n",
    "    if len(runner.dev_scores) > 0:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(dev_steps, runner.dev_scores, color=dev_color, linestyle=dev_linestyle, label='dev accuracy')\n",
    "        \n",
    "        plt.ylabel('score')\n",
    "        plt.xlabel('step')\n",
    "        plt.legend(loc=acc_legend_loc)\n",
    "    # 将绘制结果保存\n",
    "    plt.savefig(fig_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用绘图函数，将runner中的信息绘制并保存\n",
    "plot_training_loss_acc(runner, 'chapter4.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练结束后，网络的参数会自动保存为.pth结尾的文件，且与训练文件在同一目录下\n",
    "model_path = 'model_25.pth'\n",
    "# 首先读入经过训练后的网络的参数\n",
    "runner.load_model(model_path)\n",
    "x, label= next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   请分析一下预测结果，输出的三个数字分别代表三个类的得分，模型认为最后一类得分最高，即输入特征对应为最后一类，与标签值吻合。\n",
    "    在构建测试集的DataLoader类时，我们设置shuffle为True，所以可以多次尝试预测不同的结果并与标签值对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(runner.predict(x.float()))\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9ccd8639d7ac6d8ae46f08631d02de0d1c9f4a08850208985333be71082afd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
